# Resume Parser - Intelligent Multi-Strategy Pipeline

A production-ready, intelligent resume parsing system that handles PDF, DOCX, and scanned documents with automatic strategy selection and self-learning capabilities.

## ğŸš€ Features

- **Multi-Format Support**: PDF, DOCX, DOC, and scanned documents
- **Intelligent Strategy Selection**: Automatically chooses the best parsing approach
- **Self-Learning**: Improves section detection from parsed resumes
- **Batch Processing**: Process entire folders in parallel with progress tracking
- **Quality Validation**: Automatic quality scoring and validation
- **Excel Export**: Export parsed sections to Excel with clean formatting
- **Web Viewer**: View and validate parsing results in browser

## ğŸ“‹ Requirements

```bash
pip install -r requirements.txt
```

### Key Dependencies

- **PDF Processing**: `pdfplumber`, `pymupdf`
- **OCR**: `easyocr` (for scanned documents)
- **DOCX Processing**: `python-docx`
- **ML Models**: `sentence-transformers`, `spacy`
- **Export**: `openpyxl` (Excel), `pandas`

## ğŸ¯ Quick Start

### 1. Parse a Single Resume

```python
from src.core.unified_pipeline import UnifiedPipeline

pipeline = UnifiedPipeline()
result = pipeline.parse("resume.pdf", verbose=True)

print(f"Success: {result['success']}")
print(f"Strategy: {result['metadata']['strategy']}")
print(f"Sections found: {len(result['result']['sections'])}")
```

### 2. Batch Process a Folder

```bash
python batch_folder_process.py --folder "resumes/" --output "results.xlsx"
```

### 3. Test the Pipeline

```bash
python test_pipeline.py
```

### 4. View Results in Browser

```bash
python view_results.py
# Open browser to http://localhost:5000
```

## ğŸ“ Project Structure

```
resume_parser/
â”œâ”€â”€ README.md                      # This file
â”œâ”€â”€ requirements.txt               # Python dependencies
â”œâ”€â”€ config/
â”‚   â””â”€â”€ sections_database.json    # Section learning database
â”‚
â”œâ”€â”€ scripts/                       # Utility scripts
â”‚   â”œâ”€â”€ batch_folder_process.py   # Batch process resumes â†’ Excel
â”‚   â”œâ”€â”€ test_pipeline.py          # Test & verify pipeline
â”‚   â””â”€â”€ view_results.py           # Web viewer for results
â”‚
â”œâ”€â”€ src/                           # Core source code
â”‚   â”œâ”€â”€ core/                      # Main pipeline components
â”‚   â”‚   â”œâ”€â”€ unified_pipeline.py   # Main entry point (USE THIS!)
â”‚   â”‚   â”œâ”€â”€ parser.py             # Legacy parser (deprecated)
â”‚   â”‚   â”œâ”€â”€ batch_processor.py   # Batch processing logic
â”‚   â”‚   â””â”€â”€ section_learner.py   # Self-learning system
â”‚   â”‚
â”‚   â”œâ”€â”€ app/                       # Application layer
â”‚   â”‚   â”œâ”€â”€ file_detector.py     # File type detection
â”‚   â”‚   â”œâ”€â”€ layout_analyzer.py   # Layout analysis
â”‚   â”‚   â”œâ”€â”€ strategy_selector.py # Strategy selection logic
â”‚   â”‚   â””â”€â”€ quality_validator.py # Quality scoring
â”‚   â”‚
â”‚   â”œâ”€â”€ PDF_pipeline/             # PDF extraction strategies
â”‚   â”‚   â”œâ”€â”€ pipeline.py           # Standard PDF parsing
â”‚   â”‚   â”œâ”€â”€ get_words.py          # Text extraction
â”‚   â”‚   â”œâ”€â”€ get_lines.py          # Line detection
â”‚   â”‚   â”œâ”€â”€ segment_sections.py  # Section segmentation
â”‚   â”‚   â”œâ”€â”€ region_detector.py   # Multi-region layout
â”‚   â”‚   â””â”€â”€ histogram_column_detector.py  # Column detection
â”‚   â”‚
â”‚   â”œâ”€â”€ DOCX_pipeline/            # DOCX extraction
â”‚   â”‚   â””â”€â”€ pipeline.py           # DOCX parsing
â”‚   â”‚
â”‚   â”œâ”€â”€ IMG_pipeline/             # OCR for scanned docs
â”‚   â”‚   â””â”€â”€ pipeline.py           # EasyOCR-based parsing
â”‚   â”‚
â”‚   â””â”€â”€ ROBUST_pipeline/          # Advanced layout-aware parsing
â”‚       â”œâ”€â”€ README.md             # Detailed documentation
â”‚       â”œâ”€â”€ pipeline.py           # Main robust pipeline
â”‚       â”œâ”€â”€ pipeline_ocr.py       # OCR variant
â”‚       â””â”€â”€ adaptive_thresholds.py # Dynamic threshold tuning
â”‚
â”œâ”€â”€ outputs/                       # Generated Excel files
â”œâ”€â”€ freshteams_resume/            # Sample resumes (for testing)
â””â”€â”€ index.html                    # Web viewer frontend

```

## ğŸ”§ Architecture

### Pipeline Flow

```
1. File Input (PDF/DOCX/Image)
          â†“
2. File Detection & Validation
          â†“
3. Layout Analysis
   - Scanned vs. digital
   - Column structure
   - Complexity score
          â†“
4. Strategy Selection
   - PDF_NATIVE: Fast text extraction
   - PDF_HISTOGRAM: Column-aware parsing
   - PDF_REGION: Complex multi-region layouts
   - OCR: Scanned documents
   - DOCX_NATIVE: Word documents
          â†“
5. Extraction & Parsing
          â†“
6. Quality Validation
   - Section coverage
   - Text quality
   - Completeness
          â†“
7. Section Learning (Optional)
   - Learn new patterns
   - Update database
          â†“
8. Output (JSON/Excel)
```

### Key Components

#### **UnifiedPipeline** (Main Entry Point)

- Orchestrates all parsing strategies
- Handles automatic fallback
- Manages learning and caching

#### **Strategy Selector**

- Analyzes document characteristics
- Chooses optimal parsing strategy
- Handles edge cases

#### **Section Learner**

- Self-improving system
- Learns section patterns from parsed resumes
- Adapts to new resume formats

#### **Quality Validator**

- Validates extraction quality
- Triggers fallback if needed
- Provides quality scores

## ğŸ“Š Supported Resume Sections

- Contact Information
- Summary / Objective
- Skills (Technical, Soft, Languages)
- Experience / Work History
- Projects
- Education
- Certifications
- Achievements / Awards
- Publications
- Research
- Volunteer Work
- Hobbies / Interests
- References
- Declarations

## ğŸ¨ Usage Examples

### Example 1: Single Resume with Custom Config

```python
from src.core.unified_pipeline import UnifiedPipeline

pipeline = UnifiedPipeline(
    config_path="config/sections_database.json",
    enable_learning=True,
    verbose=True
)

result = pipeline.parse("resume.pdf")

# Access parsed data
sections = result['result']['sections']
for section in sections:
    print(f"\n{section['section']}:")
    for line in section['lines']:
        print(f"  {line}")
```

### Example 2: Batch Processing with Custom Options

```bash
# Process folder with 8 parallel workers
python batch_folder_process.py \
    --folder "resumes/" \
    --output "batch_results.xlsx" \
    --workers 8

# Process without recursive search
python batch_folder_process.py \
    --folder "resumes/" \
    --output "results.xlsx" \
    --no-recursive

# Quiet mode (no progress)
python batch_folder_process.py \
    --folder "resumes/" \
    --output "results.xlsx" \
    --quiet
```

### Example 3: Programmatic Batch Processing

```python
from src.core.batch_processor import BatchProcessor

processor = BatchProcessor(
    output_dir="outputs/",
    max_workers=4,
    enable_learning=True
)

results = processor.process_folder(
    folder_path="resumes/",
    recursive=True
)

# Export to Excel
processor.export_to_excel(
    results=results,
    output_path="batch_output.xlsx"
)
```

## ğŸ§ª Testing

```bash
# Run full test suite
python test_pipeline.py

# Test specific file
python -c "
from src.core.unified_pipeline import UnifiedPipeline
pipeline = UnifiedPipeline()
result = pipeline.parse('test_resume.pdf', verbose=True)
print(result)
"
```

## ğŸ› Troubleshooting

### Issue: Poor OCR Quality

**Solution**: Increase DPI for better text extraction

```python
result = pipeline.parse("resume.pdf", force_ocr=True, ocr_dpi=400)
```

### Issue: Missed Sections

**Solution**: Enable learning mode to improve detection

```python
pipeline = UnifiedPipeline(enable_learning=True)
```

### Issue: Slow Processing

**Solution**: Reduce parallel workers or disable OCR fallback

```bash
python batch_folder_process.py --folder "resumes/" --workers 2
```

## ğŸ“ˆ Performance

- **Digital PDFs**: ~0.5-2 seconds per page
- **DOCX Files**: ~0.3-1 second per file
- **Scanned PDFs**: ~3-8 seconds per page (OCR)
- **Batch Processing**: ~2-5 files/second (with 4 workers)

## ğŸ¤ Contributing

1. Follow existing code structure
2. Add tests for new features
3. Update documentation
4. Use type hints
5. Keep functions modular and single-purpose

## ğŸ“ License

Proprietary - Internal Use Only

## ğŸ”„ Changelog

### v3.0 (Current)

- Unified pipeline architecture
- Self-learning section detection
- Advanced quality validation
- Web-based result viewer

### v2.0

- Multi-strategy support
- Batch processing
- Excel export

### v1.0

- Initial PDF parsing
- Basic section detection
